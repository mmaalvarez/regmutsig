{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edccb57-abba-4f6b-b2d9-7a237c40ff1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Autoencoder\n",
    "-----------\n",
    "- Adapted from Mischan Vali-Pour's Variational Autoencoder: https://github.com/lehner-lab/RDGVassociation/tree/main/somatic_component_extraction\n",
    "- ...which is in turn adapted/inspired from https://github.com/greenelab/tybalt/blob/master/tybalt_vae.ipynb\n",
    "- See also https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "Main changes \n",
    "---\n",
    "- Now it's just a vanilla autoencoder, not \"variational\"\n",
    "- It uses a different permuted table (split into train and validation) in each epoch -- but if *n* epochs > *p* permuted tables (currently the case), it will cycle through them every *p* epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc5515-1998-42af-9753-bb5fda4d32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "# python â‰¥3.5 required\n",
    "print(sys.version)\n",
    "\n",
    "# tensorflow 1.15.5 recommended\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a2457-c270-4414-a792-8512c96727b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## functions and classes\n",
    "\n",
    "## to select optimizer\n",
    "def get_optimizer(name, learning_rate):\n",
    "    if name.lower() == 'adam':\n",
    "        return optimizers.Adam(lr=learning_rate)\n",
    "    elif name.lower() == 'sgd':\n",
    "        return optimizers.SGD(lr=learning_rate)\n",
    "    elif name.lower() == 'rmsprop':\n",
    "        return optimizers.RMSprop(lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError('Optimizer name not recognized')\n",
    "        \n",
    "## custom callback to change the permuted training and validation data pair to use in each epoch          \n",
    "class DataSwitchCallback(Callback):\n",
    "    def __init__(self, data_dict):\n",
    "        super(DataSwitchCallback, self).__init__()\n",
    "        # initialize variables at first epoch only\n",
    "        self.data_dict = data_dict # store train_validation_dfs_dict\n",
    "        self.epoch_count = 0\n",
    "    # when starting an epoch...\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # ...find out which permuted data is to be used in this epoch\n",
    "        data_index = self.epoch_count % len(self.data_dict)\n",
    "        # ...select that permuted data (train and validation)\n",
    "        self.model.train_data, self.model.val_data = self.data_dict[data_index]\n",
    "        # ...print log message\n",
    "        print(f\"Using data pair #{data_index} for epoch {self.epoch_count + 1}/{epochs}\")\n",
    "        # ...and add another epoch to the count\n",
    "        self.epoch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## input arguments\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-r', '--dataset_real', \n",
    "                    default='autoencoder_input/original_scaled.tsv',\n",
    "                    help='REAL dataset (i.e. no permutations), just [-1,1]-scaled, type directory + file name')\n",
    "parser.add_argument('-p', '--dataset_permuted', \n",
    "                    default=\"autoencoder_input/permuted_coefficients_*__epoch_*.tsv\",\n",
    "                    help='PERMUTED training+validation samples (output from 1_generate_..._validating.R), type \"DIRPATH/permuted_coefficients_*_epoch_*.tsv\". IMPORTANT: it has to always go with double quotes in the python command so that the shell does not expand the wildcards, e.g. python -p \"$dataset_permuted\", or python -p \"autoencoder_input/permuted_coefficients_*__epoch_*.tsv\"')\n",
    "parser.add_argument('-v', '--validation',\n",
    "                    default='0.1',\n",
    "                    help='random fraction of the dataset_permuted to keep aside as a validation set; required only for hyperparameter optimization')\n",
    "parser.add_argument('-n', '--num_components',\n",
    "                    default='5',\n",
    "                    help='IMPORTANT to specify: latent space dimensionality (k, size, signatures)')\n",
    "parser.add_argument('-d', '--depth',\n",
    "                    default='1',\n",
    "                    help='if depth=1 (default) there is only input, latent, and ouput layers; if depth=2, there will be 2 extra (hidden) layers: 1 between the input and the encoded layers, and 1 between the latent and the ouput layers')\n",
    "parser.add_argument('-hd', '--hidden_dim',\n",
    "                    default='2',\n",
    "                    help='however many times the size of the encoded layer to be used as the size for the 2 extra hidden layers (only if depth==1)')\n",
    "parser.add_argument('-b', '--batch_size',\n",
    "                    default='200',\n",
    "                    help='number of samples to include in each learning batch')\n",
    "parser.add_argument('-e', '--epochs',         \n",
    "                    default='200',\n",
    "                    help='how many times to cycle -- every epoch a different set of PERMUTED training+validating samples is used')\n",
    "parser.add_argument('-a', '--activation_function',         \n",
    "                    default='relu',\n",
    "                    help='activation function for all layers except the output (tanh hardcoded)')\n",
    "parser.add_argument('-w', '--weight_initializer',         \n",
    "                    default='glorot_uniform',\n",
    "                    help='distribution from which initial weights are sampled')\n",
    "parser.add_argument('-lf', '--loss_function',\n",
    "                    default='mean_squared_error',\n",
    "                    help='loss function')\n",
    "parser.add_argument('-lr', '--learning_rate',\n",
    "                    default='0.0005',\n",
    "                    help='learning rate of the Adam optimizer')\n",
    "parser.add_argument('-o', '--optimizer',\n",
    "                    default='Adam',\n",
    "                    help='optimizer algorithm')\n",
    "\n",
    "# if interactive, pass values manually\n",
    "if 'ipykernel' in sys.modules:\n",
    "    args = parser.parse_args(['-r', 'autoencoder_input/original_scaled.tsv', \\\n",
    "                              '-p', \"autoencoder_input/permuted_coefficients_*__epoch_*.tsv\", \\\n",
    "                              '-v', '0.1', \\\n",
    "                              '-n', '5', \\\n",
    "                              '-d', '1', \\\n",
    "                              '-hd', '2', \\\n",
    "                              '-b', '200', \\\n",
    "                              '-e', '200', \\\n",
    "                              '-a', 'relu', \\\n",
    "                              '-w', 'glorot_uniform', \\\n",
    "                              '-lf', 'mean_squared_error', \\\n",
    "                              '-lr', '0.0005', \\\n",
    "                              '-o', 'Adam'])\n",
    "else:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e5861-f83e-4676-a8e9-0fa8b80b5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## set data paths and hyper-parameters\n",
    "## removed the kappa and beta since they are specific to VAEs\n",
    "\n",
    "dataset_real = args.dataset_real\n",
    "dataset_permuted_list = glob(args.dataset_permuted) # this is a list of permuted tables\n",
    "validation_set_fraction = float(args.validation)\n",
    "latent_dim = int(args.num_components)\n",
    "depth = int(args.depth)\n",
    "hidden_dim = int(latent_dim)*int(args.hidden_dim) # set the 2 extra hidden layers (only added if depth==2) to be each a multiple/fraction of the size of the encoded layer\n",
    "batch_size = int(args.batch_size)\n",
    "epochs = int(args.epochs)\n",
    "activation = args.activation_function\n",
    "weight_init = args.weight_initializer\n",
    "loss_function = args.loss_function\n",
    "learning_rate = float(args.learning_rate)\n",
    "optimizer_name = args.optimizer\n",
    "optimizer = get_optimizer(optimizer_name, learning_rate)\n",
    "\n",
    "# set random seed\n",
    "seed = int(np.random.randint(low=0, high=10000, size=1))\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e98a1-11f9-444e-bc53-f4dbd4e03b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## upload and process input data\n",
    "\n",
    "## upload [-1,1]-scaled ORIGINAL data (for final signature extraction)\n",
    "real_df = pd.read_csv(dataset_real, sep='\\t')\n",
    "# store sample names column, renamed as \"Sample\"\n",
    "sample_id = real_df.drop(real_df.columns[1:], axis=1).rename(columns={'sample_id': 'Sample'})\n",
    "# store column names\n",
    "column_names = real_df.drop(real_df.columns[0], axis=1).columns\n",
    "# convert to numpy array (remove sample (first) column)\n",
    "real_df = np.array(real_df.drop(real_df.columns[0], axis=1))\n",
    "print(real_df.shape)\n",
    "\n",
    "## upload [-1,1]-scaled full PERMUTED data (for training and validation)\n",
    "# load all permuted tables, split each one into train and validation, and store in a dict; each pair will be used for a different epoch (or if n epochs > p permuted tables, just reuse them every p epochs) \n",
    "train_validation_dfs_dict = dict()\n",
    "for i,dataset_permuted in enumerate(dataset_permuted_list):\n",
    "    # load permuted table\n",
    "    full_df = pd.read_csv(dataset_permuted, sep='\\t')\n",
    "    # split it into training and validation tables, using the specified fraction for validation \n",
    "    validation_df = full_df.sample(frac=validation_set_fraction, random_state=seed)\n",
    "    train_df = full_df.drop(validation_df.index)\n",
    "    # convert to numpy arrays (remove sample (first) and nIter (last) columns)\n",
    "    train_df = np.array(train_df.drop(train_df.columns[0], axis=1).drop(train_df.columns[-1], axis=1))\n",
    "    validation_df = np.array(validation_df.drop(validation_df.columns[0], axis=1).drop(validation_df.columns[-1], axis=1))\n",
    "    # store\n",
    "    train_validation_dfs_dict[i] = [train_df, validation_df]\n",
    "    \n",
    "# extract the 1st train/validation pair to know the number of neurons needed in input layer (\"original_dim\")\n",
    "first_train_df,first_validation_df = train_validation_dfs_dict[0]\n",
    "original_dim = first_train_df.shape[1]\n",
    "print(first_train_df.shape)\n",
    "print(first_validation_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7311e54-3f85-45be-92d2-f6b291253664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ENCODER ####\n",
    "\n",
    "# set number of neurons in input layer: i.e. 1 sample's coefficients vector, with 1 coefficient per predicting feature (DNA repair marks & SBS96)\n",
    "coefficients_vector_input = Input(shape=(original_dim,))\n",
    "\n",
    "# build hidden layers (at least the latent space, i.e. encoded layer)\n",
    "if depth == 1: # default, only 1 encoded layer\n",
    "    encoded = Dense(latent_dim, \n",
    "                    activation=activation, \n",
    "                    kernel_initializer=weight_init)(coefficients_vector_input)\n",
    "elif depth == 2: # 1 hidden + the encoded layers\n",
    "    hidden_dense = Dense(hidden_dim, \n",
    "                         activation=activation,\n",
    "                         kernel_initializer=weight_init)(coefficients_vector_input)\n",
    "    encoded = Dense(latent_dim, \n",
    "                    activation=activation, \n",
    "                    kernel_initializer=weight_init)(hidden_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c56cc-c663-455a-82da-72be6a61c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECODER ####\n",
    "\n",
    "# build at least the output (\"decoded\") layer\n",
    "if depth == 1: # default\n",
    "    decoded = Dense(original_dim, \n",
    "                    # tanh hardcoded as activation function since our output values have to be between -1 and 1, like the input\n",
    "                    activation='tanh', \n",
    "                    kernel_initializer=weight_init)(encoded)\n",
    "elif depth == 2: # 1 hidden + 1 output layers\n",
    "    hidden_decoded = Dense(hidden_dim, \n",
    "                           activation=activation, \n",
    "                           kernel_initializer=weight_init)(encoded)\n",
    "    decoded = Dense(original_dim, \n",
    "                    activation='tanh', \n",
    "                    kernel_initializer=weight_init)(hidden_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce444f21-51fc-400a-88b9-bf9eb9552a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### build autoencoder and encoder models ###############\n",
    "## the 'autoencoder' and 'encoder' models are \"intertwined through shared layer references. When you train the autoencoder, you're indirectly training the encoder because they share the same initial layers, and these layers have associated weights and biases. Since the encoder shares these layers with the autoencoder, any updates to the weights and biases in the autoencoder are reflected in the encoder. So after training, you can use the encoder separately to obtain the compressed (encoded) representations of data without reconstructing it, even though the training process involved both encoding and decoding\"\n",
    "\n",
    "## autoencoder model: map the input layer to its reconstruction (i.e. to the output layer)\n",
    "autoencoder = Model(coefficients_vector_input, decoded)\n",
    "\n",
    "# use optimizer to backpropagate based on the loss\n",
    "autoencoder.compile(optimizer=optimizer, \n",
    "                    loss=loss_function) # mean squared error as loss function; 'binary_crossentropy' would yield higher losses\n",
    "\n",
    "# Add a reference to the training and validation data in the autoencoder model\n",
    "autoencoder.train_data = first_train_df\n",
    "autoencoder.val_data = first_validation_df\n",
    "\n",
    "# summary of autoencoder model\n",
    "autoencoder.summary()\n",
    "\n",
    "\n",
    "## encoder model: map the input layer to its encoded representation (i.e. to the latent space)\n",
    "# this will be used after training\n",
    "encoder = Model(coefficients_vector_input, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caae779-8d23-4674-931c-4e55c61ac309",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### train (fit) the 'autoencoder' model ##########################\n",
    "\n",
    "# Create an instance of the callback with the data dictionary\n",
    "data_switcher = DataSwitchCallback(train_validation_dfs_dict)\n",
    "\n",
    "# Run the training\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(epochs):\n",
    "    # use a different train_data + val_data pair at each iteration (i.e. epoch)\n",
    "    epoch_it = autoencoder.fit(autoencoder.train_data, autoencoder.train_data,\n",
    "                               shuffle = True,\n",
    "                               epochs = 1, # run only one epoch at each loop iteration\n",
    "                               batch_size = batch_size,\n",
    "                               validation_data = (autoencoder.val_data, autoencoder.val_data),\n",
    "                               callbacks = [data_switcher])\n",
    "    # keep track of losses\n",
    "    history['loss'].append(epoch_it.history['loss'][0])\n",
    "    history['val_loss'].append(epoch_it.history['val_loss'][0])\n",
    "\n",
    "# evaluate final loss\n",
    "training_loss = autoencoder.evaluate(autoencoder.train_data, autoencoder.train_data)\n",
    "validation_loss = autoencoder.evaluate(autoencoder.val_data, autoencoder.val_data)\n",
    "print(f'Final training loss: {training_loss.round(2)}')\n",
    "print(f'Final validation loss: {validation_loss.round(2)}')\n",
    "\n",
    "# visualize training performance\n",
    "history_df = pd.DataFrame(history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Autoencoder loss')\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9c19f-c885-49c4-9b73-1fa99eb9f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### now use the 'encoder' model (already trained via the autoencoder, see above) to encode the original (i.e. not permuted) -1,1 scaled coefficients matrix into the latent representation\n",
    "## these k values should roughly correspond to each sample's k signature exposures from the NMF\n",
    "\n",
    "encoded_real_df = encoder.predict_on_batch(real_df)\n",
    "\n",
    "# rename columns ('signatures'), and convert to pandas df\n",
    "encoded_real_df = pd.DataFrame(encoded_real_df, columns = range(1, latent_dim+1)).add_prefix('ae')\n",
    "\n",
    "# append sample names column\n",
    "encoded_real_df = pd.concat([sample_id, encoded_real_df], axis=1)\n",
    "\n",
    "## check nodes activity to ensure that the model is learning a distribution of feature activations, and not zeroing out features\n",
    "sum_node_activity = encoded_real_df.drop('Sample',axis=1).sum(axis=0).sort_values(ascending=False)\n",
    "sum_node_activity_mean = sum_node_activity.mean()\n",
    "print(sum_node_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721c5f5-8d69-4b11-9bff-8d2fc81e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### save outputs ###############\n",
    "\n",
    "output_folder_name = f'{str(validation_set_fraction)}-validation_' \\\n",
    "                     f'{str(latent_dim)}-components_' \\\n",
    "                     f'{str(depth)}-depth_' \\\n",
    "                     f'{str(hidden_dim)}-hiddendim_' \\\n",
    "                     f'{str(batch_size)}-batchsize_' \\\n",
    "                     f'{str(epochs)}-epochs_' \\\n",
    "                     f'{activation}_' \\\n",
    "                     f'{weight_init}_' \\\n",
    "                     f'{loss_function}_' \\\n",
    "                     f'{str(learning_rate)}-learningrate_' \\\n",
    "                     f'{optimizer_name}_' \\\n",
    "                     f'{str(round(sum_node_activity_mean,2))}-meansumactivity_' \\\n",
    "                     f'{str(seed)}-seed/'\n",
    "\n",
    "## if interactive\n",
    "if 'ipykernel' in sys.modules:\n",
    "    if not os.path.exists('autoencoder_output'):\n",
    "        os.mkdir('autoencoder_output')\n",
    "    output_folder_name = 'autoencoder_output/' + output_folder_name\n",
    "    os.mkdir(output_folder_name)\n",
    "else:\n",
    "    ## not interactive (nextflow handles the autoencoder_output folder creation)\n",
    "    os.mkdir(output_folder_name)\n",
    "    \n",
    "# training performance plot\n",
    "fig.savefig(output_folder_name + 'loss_history.jpg', dpi=600)    \n",
    "\n",
    "# viz network\n",
    "plot_model(autoencoder,\n",
    "           to_file=output_folder_name + 'model_viz.jpg',\n",
    "           show_layer_names=False,\n",
    "           show_shapes=True,\n",
    "           rankdir='TB', #\"LR\" horizontal\n",
    "           dpi=600)\n",
    "\n",
    "# encoder model\n",
    "encoder.save(output_folder_name + 'encoder_model.tf')\n",
    "\n",
    "# encoded layer (\"signature exposures\")\n",
    "encoded_real_df.to_csv(output_folder_name + 'encoded_layer.tsv', sep='\\t', index= False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
